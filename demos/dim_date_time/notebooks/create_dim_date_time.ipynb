{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5819a-bed5-483d-9125-24f4d50cf722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Dimensions for Date and Time\n",
    "\n",
    "## Description\n",
    "- Create table for date dimension\n",
    "- Create table for time dimension in minutes\n",
    "- Create table which combines the dimensions date and timeminute in one table\n",
    "- Create days_off table\n",
    "- Append the compensation_holidays to date dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdaeca9f-1ed1-4271-b1ad-18129ab414f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ToDo: Remove Hardcoded code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71360457-4bdd-4775-8ef4-d0f0131c249e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65161b8e-63c2-4fe9-a555-c405b3264f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../utilities/altyca_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6fef1cd-d34e-4819-b8e8-149aa07bc9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alu = Utility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb56c415-dbd5-4d89-8b12-78035c51e6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "190079fb-74ab-4411-a3c3-540bce1db182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"altyca_demo\", \"Catalog\")\n",
    "dbutils.widgets.text(\"start_date\", \"2020-01-01\", \"Start Date\")\n",
    "dbutils.widgets.text(\"end_date\", \"2030-12-31\", \"End Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a3e3802-925d-4b61-89e0-a0f0b2bd5f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "environment = alu.get_environment()\n",
    "# catalog = f\"{dbutils.widgets.get('catalog')}_{environment}\"\n",
    "catalog = f\"{dbutils.widgets.get('catalog')}\"\n",
    "start_date = dbutils.widgets.get(\"start_date\")\n",
    "end_date = dbutils.widgets.get(\"end_date\")\n",
    "\n",
    "schema = \"shared_dimensions\"\n",
    "dim_date_table = f\"{catalog}.{schema}.dim_date\"\n",
    "days_off_table = f\"{catalog}.{schema}.days_off\"\n",
    "time_minute_table = f\"{catalog}.{schema}.dim_time_minute\"\n",
    "mapping_minute_table = f\"{catalog}.{schema}.dim_mapping_minute\"\n",
    "comment = f\"This object is created using the Job Dim Date Deploy Job and contains the date up to date {start_date} to {end_date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9df7f18-6ffe-4741-8604-bf1c7285e2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Create Dimension Tables for Date and Time\")\n",
    "print(f\"Timerange: {start_date} to {end_date}\")\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Dim Date Table: {dim_date_table}\")\n",
    "print(f\"Days Off Table: {days_off_table}\")\n",
    "print(f\"Time Minute Table: {time_minute_table}\")\n",
    "print(f\"Mapping Minute Table: {mapping_minute_table}\")\n",
    "print(f\"This Notebook runs in Environment: {environment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c68b34-d041-46a8-9b7d-551845d4591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Translation and abbreviated translation for German, French, and Italian --> create more translations as wished\n",
    "months_dict = {\n",
    "    \"January\": [\"Januar\", \"Jan.\", \"janvier\", \"janv.\", \"gennaio\", \"gen.\"],\n",
    "    \"February\": [\"Februar\", \"Feb.\", \"février\", \"fév.\", \"febbraio\", \"feb.\"],\n",
    "    \"March\": [\"März\", \"Mär.\", \"mars\", \"mar.\", \"marzo\", \"mar.\"],\n",
    "    \"April\": [\"April\", \"Apr.\", \"avril\", \"avr.\", \"aprile\", \"apr.\"],\n",
    "    \"May\": [\"Mai\", \"Mai.\", \"mai\", \"mai.\", \"maggio\", \"mag.\"],\n",
    "    \"June\": [\"Juni\", \"Jun.\", \"juin\", \"jui.\", \"giugno\", \"giu.\"],\n",
    "    \"July\": [\"Juli\", \"Jul.\", \"juillet\", \"juil.\", \"luglio\", \"lug.\"],\n",
    "    \"August\": [\"August\", \"Aug.\", \"août\", \"aoû.\", \"agosto\", \"ago.\"],\n",
    "    \"September\": [\"September\", \"Sep.\", \"septembre\", \"sept.\", \"settembre\", \"set.\"],\n",
    "    \"October\": [\"Oktober\", \"Okt.\", \"octobre\", \"oct.\", \"ottobre\", \"ott.\"],\n",
    "    \"November\": [\"November\", \"Nov.\", \"novembre\", \"nov.\", \"novembre\", \"nov.\"],\n",
    "    \"December\": [\"Dezember\", \"Dez.\", \"décembre\", \"déc.\", \"dicembre\", \"dic.\"],\n",
    "}\n",
    "\n",
    "# Translation and abbreviated translation for German, French, and Italian\n",
    "days_dict = {\n",
    "    \"Monday\": [\"Montag\", \"Mo.\", \"lundi\", \"lun.\", \"lunedì\", \"lun.\"],\n",
    "    \"Tuesday\": [\"Dienstag\", \"Di.\", \"mardi\", \"mar.\", \"martedì\", \"mar.\"],\n",
    "    \"Wednesday\": [\"Mittwoch\", \"Mi.\", \"mercredi\", \"mer.\", \"mercoledì\", \"mer.\"],\n",
    "    \"Thursday\": [\"Donnerstag\", \"Do.\", \"jeudi\", \"jeu.\", \"giovedì\", \"gio.\"],\n",
    "    \"Friday\": [\"Freitag\", \"Fr.\", \"vendredi\", \"ven.\", \"venerdì\", \"ven.\"],\n",
    "    \"Saturday\": [\"Samstag\", \"Sa.\", \"samedi\", \"sam.\", \"sabato\", \"sab.\"],\n",
    "    \"Sunday\": [\"Sonntag\", \"So.\", \"dimanche\", \"dim.\", \"domenica\", \"dom.\"],\n",
    "}\n",
    "\n",
    "date_mapping_dict = {\"Months\": months_dict, \"Days\": days_dict}\n",
    "\n",
    "\n",
    "# Define the nested mapping dictionary for different window groups\n",
    "time_mapping_dict = {\n",
    "    \"5Min\": {\n",
    "        1: \"00-05\",\n",
    "        2: \"05-10\",\n",
    "        3: \"10-15\",\n",
    "        4: \"15-20\",\n",
    "        5: \"20-25\",\n",
    "        6: \"25-30\",\n",
    "        7: \"30-35\",\n",
    "        8: \"35-40\",\n",
    "        9: \"40-45\",\n",
    "        10: \"45-50\",\n",
    "        11: \"50-55\",\n",
    "        12: \"55-60\",\n",
    "    },\n",
    "    \"10Min\": {1: \"00-10\", 2: \"10-20\", 3: \"20-30\", 4: \"30-40\", 5: \"40-50\", 6: \"50-60\"},\n",
    "    \"15Min\": {1: \"00-15\", 2: \"15-30\", 3: \"30-45\", 4: \"45-60\"},\n",
    "    \"20Min\": {1: \"00-20\", 2: \"20-40\", 3: \"40-60\"},\n",
    "    \"30Min\": {1: \"00-30\", 2: \"30-60\"},\n",
    "    \"60Min\": {1: \"00-60\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a979a639-46e0-409d-b82e-3ad6e78efa89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb6d835-ea07-4a0a-924c-3b7dc1fd7b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a UDF to map the time values to the corresponding window group\n",
    "get_window_group = udf(\n",
    "    lambda minute, window_type: time_mapping_dict[window_type][\n",
    "        next(\n",
    "            (\n",
    "                group\n",
    "                for group, window in time_mapping_dict[window_type].items()\n",
    "                if int(window.split(\"-\")[0]) <= minute < int(window.split(\"-\")[1])\n",
    "            ),\n",
    "            0,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# function to create time windows\n",
    "def create_time_window(\n",
    "    df,\n",
    "    duration,\n",
    "    timestampCol=\"TimestampStartUtc\",\n",
    "    minuteCol=\"TimeMinute\",\n",
    "    secondCol=\"TimeSecond\",\n",
    "):\n",
    "    \"\"\"\n",
    "    to a given dataframe this function add 3 columns:\n",
    "    - if a minute is a start of a window: true or false\n",
    "    - an Id of the window group: int\n",
    "    - a name for the window group: string\n",
    "    \"\"\"\n",
    "\n",
    "    is_window_start = f\"IsWindow{duration}MinuteStart\"\n",
    "    window_id = f\"Window{duration}MinuteId\"\n",
    "    window_name = f\"Window{duration}MinuteName\"\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            is_window_start,\n",
    "            when(\n",
    "                (col(minuteCol) % duration == 0) & (col(secondCol) == 0), True\n",
    "            ).otherwise(False),\n",
    "        )\n",
    "        .withColumn(window_id, ceil((col(minuteCol) + 1) / duration).cast(\"integer\"))\n",
    "        .withColumn(window_name, get_window_group(minuteCol, lit(f\"{duration}Min\")))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_date_columns(df):\n",
    "    \"\"\"\n",
    "    function to add date related columns\n",
    "    The function returns the calculated dataframe and the added date_columns\n",
    "    \"\"\"\n",
    "    # date\n",
    "    df = df.withColumn(\n",
    "        \"DatePK\", expr(\"year(Date) * 10000 + month(Date) * 100 + day(Date)\")\n",
    "    ).withColumn(\"DateString\", expr(\"date_format(Date, 'dd.MM.yyyy')\"))\n",
    "\n",
    "    # year\n",
    "    df = (\n",
    "        df.withColumn(\"Year\", year(col(\"Date\"))).withColumn(\n",
    "            \"IsLeapYear\",\n",
    "            when(\n",
    "                (expr(\"year(Date) % 4 == 0\") & expr(\"year(Date) % 100 != 0\"))\n",
    "                | expr(\"year(Date) % 400 == 0\"),\n",
    "                True,\n",
    "            ).otherwise(False),\n",
    "        )\n",
    "        # How to calculate leap year: https://learn.microsoft.com/en-us/office/troubleshoot/excel/determine-a-leap-year\n",
    "    )\n",
    "\n",
    "    # halfyear\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            \"HalfYearId\", when(month(df[\"Date\"]).between(1, 6), 1).otherwise(2)\n",
    "        )\n",
    "        .withColumn(\"YearHalfYearId\", col(\"Year\") * 10 + col(\"HalfYearId\"))\n",
    "        .withColumn(\n",
    "            \"HalfYearName\", when(month(df[\"Date\"]).between(1, 6), \"H1\").otherwise(\"H2\")\n",
    "        )\n",
    "        .withColumn(\"YearHalfYear\", expr(\"concat(Year,'-', HalfYearName)\"))\n",
    "    )\n",
    "\n",
    "    # quarter\n",
    "    df = (\n",
    "        df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\n",
    "        .withColumn(\"YearQuarterId\", expr(\"year(Date) * 10 + quarter(Date)\"))\n",
    "        .withColumn(\"YearQuarter\", expr(\"concat(Year,'-Q', quarter(Date))\"))\n",
    "        .withColumn(\"QuarterShortname\", expr(\"concat('Q', quarter(Date))\"))\n",
    "    )\n",
    "\n",
    "    # month\n",
    "    df = (\n",
    "        df.withColumn(\"YearMonthId\", expr(\"CAST(date_format(Date, 'yyyyMM') AS INT)\"))\n",
    "        .withColumn(\"Month\", month(\"Date\"))\n",
    "        .withColumn(\"MonthName\", expr(\"date_format(Date, 'MMMM')\"))\n",
    "        .withColumn(\"MonthNameShort\", expr(\"date_format(Date, 'MMM')\"))\n",
    "        .withColumn(\n",
    "            \"MonthNameDe\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN MonthName == '{0}' THEN '{1}'\".format(k, v[0])\n",
    "                        for k, v in date_mapping_dict[\"Months\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MonthNameShortDe\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN MonthName == '{0}' THEN '{1}'\".format(k, v[1])\n",
    "                        for k, v in date_mapping_dict[\"Months\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MonthNameFr\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN MonthName == '{0}' THEN '{1}'\".format(k, v[2])\n",
    "                        for k, v in date_mapping_dict[\"Months\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MonthNameShortFr\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN MonthName == '{0}' THEN '{1}'\".format(k, v[3])\n",
    "                        for k, v in date_mapping_dict[\"Months\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MonthNameIt\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN MonthName == '{0}' THEN '{1}'\".format(k, v[4])\n",
    "                        for k, v in date_mapping_dict[\"Months\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MonthNameShortIt\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN MonthName == '{0}' THEN '{1}'\".format(k, v[5])\n",
    "                        for k, v in date_mapping_dict[\"Months\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # week\n",
    "    df = (\n",
    "        df.withColumn(\"WeekIdIso\", expr(\"year(Date) * 100 + extract(WEEKS FROM Date)\"))\n",
    "        .withColumn(\"WeekOfYearIso\", expr(\"extract(WEEKS FROM Date)\"))\n",
    "        .withColumn(\"WeekOfYearIsoName\", expr(\"concat('W', WeekOfYearIso)\"))\n",
    "        # the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004,  while 2012-12-31 is part of the first week of 2013.\n",
    "        .withColumn(\"YearWeekOfYearIso\", expr(\"concat(Year,'-', WeekOfYearIsoName)\"))\n",
    "    )\n",
    "\n",
    "    # day\n",
    "    df = (\n",
    "        df.withColumn(\"DayName\", expr(\"date_format(Date, 'EEEE')\"))\n",
    "        .withColumn(\"DayNameShort\", expr(\"date_format(Date, 'EEE')\"))\n",
    "        .withColumn(\n",
    "            \"DayNameDe\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN DayName == '{0}' THEN '{1}'\".format(k, v[0])\n",
    "                        for k, v in date_mapping_dict[\"Days\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DayNameShortDe\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN DayName == '{0}' THEN '{1}'\".format(k, v[1])\n",
    "                        for k, v in date_mapping_dict[\"Days\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DayNameFr\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN DayName == '{0}' THEN '{1}'\".format(k, v[2])\n",
    "                        for k, v in date_mapping_dict[\"Days\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DayNameShortFr\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN DayName == '{0}' THEN '{1}'\".format(k, v[3])\n",
    "                        for k, v in date_mapping_dict[\"Days\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DayNameIt\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN DayName == '{0}' THEN '{1}'\".format(k, v[4])\n",
    "                        for k, v in date_mapping_dict[\"Days\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DayNameShortIt\",\n",
    "            expr(\n",
    "                \"CASE \"\n",
    "                + \" \".join(\n",
    "                    [\n",
    "                        \"WHEN DayName == '{0}' THEN '{1}'\".format(k, v[5])\n",
    "                        for k, v in date_mapping_dict[\"Days\"].items()\n",
    "                    ]\n",
    "                )\n",
    "                + \" END\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"DayOfYear\", expr(\"dayofyear(Date)\"))\n",
    "        .withColumn(\"Day\", expr(\"dayofmonth(Date)\"))\n",
    "        .withColumn(\"DayOfWeekUs\", expr(\"extract(DOW FROM Date)\"))\n",
    "        .withColumn(\"DayOfWeekIso\", expr(\"extract(DOW_ISO FROM Date)\"))\n",
    "        .withColumn(\"IsWeekDay\", expr(\"DayOfWeekIso < 6\"))\n",
    "        .withColumn(\"IsLastDayOfMonth\", expr(\"Date = last_day(Date)\"))\n",
    "        .withColumn(\"LastDayOfMonth\", expr(\"last_day(Date)\"))\n",
    "        .withColumn(\"MonthDay\", expr(\"make_date(1972, Month, Day)\"))\n",
    "        .withColumn(\"StartOfWeekUs\", expr(\"date_sub(Date, DayOfWeekUs-1)\"))\n",
    "        .withColumn(\"StartOfWeekIso\", expr(\"date_sub(Date, DayOfWeekIso-1)\"))\n",
    "        .withColumn(\"EndOfWeekUs\", expr(\"date_add(Date, 7-DayOfWeekUs)\"))\n",
    "        .withColumn(\"EndOfWeekIso\", expr(\"date_add(Date, 7-DayOfWeekIso)\"))\n",
    "        .withColumn(\"IsCompensation\", lit(None).cast(\"boolean\"))\n",
    "        .withColumn(\"IsHoliday\", lit(None).cast(\"boolean\"))\n",
    "    )\n",
    "\n",
    "    date_columns = df.columns\n",
    "\n",
    "    # return results\n",
    "    return df, date_columns\n",
    "\n",
    "\n",
    "def add_time_columns(df):\n",
    "    \"\"\"\n",
    "    function to add time related columns\n",
    "    The function returns the calculated dataframe and the added time_columns\n",
    "    \"\"\"\n",
    "    # generate timestamps from TimePK\n",
    "    df = (\n",
    "        df.withColumn(\"TimestampStartUtc\", col(\"TimestampPK\").cast(\"timestamp\"))\n",
    "        # .withColumn(\"TimestampStartCet\", from_utc_timestamp(\"TimestampStartUtc\", \"Europe/Zurich\"))\n",
    "        .withColumn(\"TimePK\", expr(\"TimestampPK%86400\"))\n",
    "        .withColumn(\"Time\", col(\"TimePK\").cast(\"timestamp\"))\n",
    "        .withColumn(\"TimeString\", date_format(col(\"Time\"), \"HH:mm:ss\"))\n",
    "    )\n",
    "\n",
    "    # add time columns\n",
    "    df = (\n",
    "        df.withColumn(\"TimeHour\", hour(\"Time\"))\n",
    "        .withColumn(\"TimeMinute\", minute(\"Time\"))\n",
    "        .withColumn(\"TimeSecond\", second(\"Time\"))\n",
    "        .withColumn(\"AmPm\", date_format(\"Time\", \"a\"))\n",
    "    )\n",
    "\n",
    "    # add minute and second identifiers\n",
    "    df = df.withColumn(\n",
    "        \"TimeSecondId\", expr(\"TimeHour*10000+TimeMinute*100+TimeSecond\")\n",
    "    ).withColumn(\"TimeMinuteId\", expr(\"TimeHour*100+TimeMinute\"))\n",
    "\n",
    "    # Create windows\n",
    "    windows = [5, 10, 15, 20, 30, 60]\n",
    "    for window in windows:\n",
    "        df = create_time_window(df, window)\n",
    "\n",
    "    time_columns = df.columns\n",
    "\n",
    "    # return results\n",
    "    return df, time_columns\n",
    "\n",
    "\n",
    "def expand_date_to_datetime_table(df_date, resolution_in_seconds=1):\n",
    "    \"\"\"\n",
    "    given an input dataframe df_date for a certain date range\n",
    "    we calculate the dateTime table with <resolution_in_seconds>.\n",
    "    The function returns the calculated dataframe and the added time_columns\n",
    "    \"\"\"\n",
    "    date_to_seconds_range = (\n",
    "        df_date.select(to_timestamp(\"Date\").cast(LongType()).alias(\"Seconds\"))\n",
    "        .groupBy(lit(1))\n",
    "        .agg(min(\"Seconds\").alias(\"min\"), max(\"Seconds\").alias(\"max\"))\n",
    "        .select(\"min\", \"max\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    # Create range for all seconds in 24h\n",
    "    df = spark.range(\n",
    "        date_to_seconds_range.min, date_to_seconds_range.max + 1, resolution_in_seconds\n",
    "    ).select(col(\"id\").cast(LongType()).alias(\"TimestampPK\"))\n",
    "\n",
    "    # add time columns\n",
    "    df, time_columns = add_time_columns(df)\n",
    "\n",
    "    # add date columns\n",
    "    df, _ = add_date_columns(df.withColumn(\"Date\", to_date(\"TimestampStartUtc\")))\n",
    "\n",
    "    # rename the Date and WeekTime PK to FK (Foreign Key)\n",
    "    df = df.withColumnRenamed(\"DatePK\", \"DateUtcFK\").withColumnRenamed(\n",
    "        \"WeekTimeUtcPK\", \"WeekTimeUtcFK\"\n",
    "    )\n",
    "\n",
    "    # add local date and week time columns\n",
    "    df = (\n",
    "        df\n",
    "        # .withColumn(\"DateCet\", to_date(\"TimestampStartCet\"))\n",
    "        # .withColumn(\"DateCetPK\", expr(\"year(DateCet) * 10000 + month(DateCet) * 100 + day(DateCet)\"))\n",
    "        .withColumn(\"WeekTimePK\", (col(\"DayOfWeekIso\") - 1) * 86400 + col(\"TimePK\"))\n",
    "        .withColumn(\n",
    "            \"WeekTime\",\n",
    "            to_timestamp(\n",
    "                to_timestamp(lit(\"1970-01-05\")).cast(LongType()) + col(\"WeekTimePK\")\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"WeekTimeDuration\", col(\"WeekTimePK\") / 86400.0)\n",
    "        .withColumn(\"DayTimeDuration\", col(\"TimePK\") / 86400.0)\n",
    "    )\n",
    "\n",
    "    # return results\n",
    "    return df, time_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42650267-61f1-4020-a5c1-5a2bf131249c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create dimension date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228af06b-1066-490c-ae9f-bc47c816440a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataframe with date interval\n",
    "df_date_range = spark.sql(\n",
    "    f\"SELECT EXPLODE(SEQUENCE(to_date('{start_date}'), to_date('{end_date}'), INTERVAL 1 DAY)) AS Date\"\n",
    ")\n",
    "\n",
    "# add date columns\n",
    "df_date_range, date_columns = add_date_columns(df_date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "734794e1-3889-4b52-bc44-d175be997bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56f18f4-b339-4bb0-a5ff-ea524555db2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create dimension time in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dd1dd3f-f301-4426-bcd8-12a7cfb0b6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_time_range_seconds, time_columns = expand_date_to_datetime_table(\n",
    "    df_date=df_date_range, resolution_in_seconds=1\n",
    ")\n",
    "df_time_range_minutes, time_columns = expand_date_to_datetime_table(\n",
    "    df_date=df_date_range, resolution_in_seconds=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2f0031-a824-4258-88ec-fb3f6cefa1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Mapping Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c03c87-eea8-48e7-93aa-4876a47d9619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_timestamp_columns = [\n",
    "    \"DateUtcFK\",\n",
    "    \"TimestampStartUtc\",\n",
    "    \"WeekTimePK\",\n",
    "] \n",
    "\n",
    "df_mapping_minutes = df_time_range_minutes.select(date_timestamp_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d068511-6fd4-4630-af6f-9a35cb7c06c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Time Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca799df0-5195-4c63-96f0-da2070e111d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "week_time_filter_query = f\"Year=year('{start_date}') AND Month=month('{start_date}')\"\n",
    "\n",
    "df_time_seconds = (\n",
    "    df_time_range_seconds.filter(week_time_filter_query)\n",
    "    .select(\n",
    "        [\"WeekTimePK\", \"WeekTime\", \"WeekTimeDuration\", \"DayTimeDuration\"] + time_columns\n",
    "    )\n",
    "    .drop(\"TimestampPK\", \"TimestampStartUtc\")  # , 'TimestampStartCet'\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "df_time_minutes = (\n",
    "    df_time_range_minutes.filter(week_time_filter_query)\n",
    "    .select(df_time_seconds.columns)\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dd78b79-b901-4389-b676-390aa597eee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists and create new with comment\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {dim_date_table}\")\n",
    "\n",
    "df_date_range.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").option(\"comment\", comment).saveAsTable(dim_date_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad003a3-def1-438c-a0c7-e7a00c1ad094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists and create new with comment\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {time_minute_table}\")\n",
    "df_time_minutes.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(time_minute_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97080d92-124b-4669-b034-e80bba513d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists and create new with comment\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {mapping_minute_table}\")\n",
    "\n",
    "df_mapping_minutes.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").option(\"comment\", comment).saveAsTable(mapping_minute_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d113c96c-c3ae-47a8-aa68-e29d723597ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f0b197-c620-4403-a75e-8d0f1b47fdf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_minute \n",
    "COMMENT '{comment}' AS\n",
    "  SELECT\n",
    "    dd.Date,\n",
    "    dd.DatePK,\n",
    "    dd.DateString,\n",
    "    dd.Year,\n",
    "    dd.IsLeapYear,\n",
    "    dd.HalfYearId,\n",
    "    dd.YearHalfYearId,\n",
    "    dd.HalfYearName,\n",
    "    dd.YearHalfYear,\n",
    "    dd.Quarter,\n",
    "    dd.YearQuarterId,\n",
    "    dd.YearQuarter,\n",
    "    dd.QuarterShortname,\n",
    "    dd.YearMonthId,\n",
    "    dd.Month,\n",
    "    dd.MonthName,\n",
    "    dd.MonthNameShort,\n",
    "    dd.MonthNameDe,\n",
    "    dd.MonthNameShortDe,\n",
    "    dd.MonthNameFr,\n",
    "    dd.MonthNameShortFr,\n",
    "    dd.MonthNameIt,\n",
    "    dd.MonthNameShortIt,\n",
    "    dd.WeekIdIso,\n",
    "    dd.WeekOfYearIso,\n",
    "    dd.WeekOfYearIsoName,\n",
    "    dd.YearWeekOfYearIso,\n",
    "    dd.DayName,\n",
    "    dd.DayNameShort,\n",
    "    dd.DayNameDe,\n",
    "    dd.DayNameShortDe,\n",
    "    dd.DayNameFr,\n",
    "    dd.DayNameShortFr,\n",
    "    dd.DayNameIt,\n",
    "    dd.DayNameShortIt,\n",
    "    dd.DayOfYear,\n",
    "    dd.Day,\n",
    "    dd.DayOfWeekUs,\n",
    "    dd.DayOfWeekIso,\n",
    "    dd.IsWeekDay,\n",
    "    dd.IsLastDayOfMonth,\n",
    "    dd.LastDayOfMonth,\n",
    "    dd.MonthDay,\n",
    "    dd.StartOfWeekUs,\n",
    "    dd.StartOfWeekIso,\n",
    "    dd.EndOfWeekUs,\n",
    "    dd.EndOfWeekIso,\n",
    "    dd.IsCompensation,\n",
    "    dd.IsHoliday,\n",
    "\n",
    "    tm.WeekTimePK,\n",
    "    tm.WeekTime,\n",
    "    tm.WeekTimeDuration,\n",
    "    tm.DayTimeDuration,\n",
    "    tm.TimePK,\n",
    "    tm.Time,\n",
    "    tm.TimeString,\n",
    "    tm.TimeHour,\n",
    "    tm.TimeMinute,\n",
    "    tm.TimeSecond,\n",
    "    tm.AmPm,\n",
    "    tm.TimeSecondId,\n",
    "    tm.TimeMinuteId,\n",
    "    tm.IsWindow5MinuteStart,\n",
    "    tm.Window5MinuteId,\n",
    "    tm.Window5MinuteName,\n",
    "    tm.IsWindow10MinuteStart,\n",
    "    tm.Window10MinuteId,\n",
    "    tm.Window10MinuteName,\n",
    "    tm.IsWindow15MinuteStart,\n",
    "    tm.Window15MinuteId,\n",
    "    tm.Window15MinuteName,\n",
    "    tm.IsWindow20MinuteStart,\n",
    "    tm.Window20MinuteId,\n",
    "    tm.Window20MinuteName,\n",
    "    tm.IsWindow30MinuteStart,\n",
    "    tm.Window30MinuteId,\n",
    "    tm.Window30MinuteName,\n",
    "    tm.IsWindow60MinuteStart,\n",
    "    tm.Window60MinuteId,\n",
    "    tm.Window60MinuteName\n",
    "\n",
    "  FROM {catalog}.{schema}.dim_date AS dd\n",
    "  LEFT OUTER JOIN {catalog}.{schema}.dim_mapping_minute AS mm ON dd.DatePK = mm.DateUtcFK\n",
    "  LEFT OUTER JOIN {catalog}.{schema}.dim_time_minute AS tm ON mm.WeekTimePK = tm.WeekTimePK\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edf5970d-42fe-4335-9756-090dcc3f31f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_5_minute \n",
    "COMMENT '{comment}' AS\n",
    "SELECT * FROM {catalog}.{schema}.vw_dim_date_minute\n",
    "WHERE IsWindow5MinuteStart = true\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356f0929-1a54-4e70-8159-c292286b5148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_10_minute \n",
    "COMMENT '{comment}' AS\n",
    "SELECT * FROM {catalog}.{schema}.vw_dim_date_minute\n",
    "WHERE IsWindow10MinuteStart = true\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ef27c6-5dc9-4acf-9c13-431c56419272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_15_minute \n",
    "COMMENT '{comment}' AS\n",
    "SELECT * FROM {catalog}.{schema}.vw_dim_date_minute\n",
    "WHERE IsWindow15MinuteStart = true\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40710f84-7c5e-473b-ae46-3302b761d706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_20_minute \n",
    "COMMENT '{comment}' AS\n",
    "SELECT * FROM {catalog}.{schema}.vw_dim_date_minute\n",
    "WHERE IsWindow20MinuteStart = true\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b043a6f4-b846-43d2-b69e-266b3db91829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_30_minute \n",
    "COMMENT '{comment}' AS\n",
    "SELECT * FROM {catalog}.{schema}.vw_dim_date_minute\n",
    "WHERE IsWindow30MinuteStart = true\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae4a0af8-64a3-4d0e-9cd0-6198ffc5e591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {catalog}.{schema}.vw_dim_date_60_minute \n",
    "COMMENT '{comment}' AS\n",
    "SELECT * FROM {catalog}.{schema}.vw_dim_date_minute\n",
    "WHERE IsWindow60MinuteStart = true\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "628417e7-9da3-448d-99a1-c4a0d8cf804a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Append the days off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636e2388-2a11-4fa3-b835-5b6eb0212da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Create Days-Off Table: {days_off_table}\")\n",
    "\n",
    "sql = f\"\"\"\n",
    "    DROP TABLE IF EXISTS {days_off_table}\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f0f0fa-3e8a-41cb-8d4c-60d0b797fb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {days_off_table} (\n",
    "        Id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "        Date DATE, \n",
    "        Type STRING, \n",
    "        Comment STRING,\n",
    "        \n",
    "        PRIMARY KEY (Id)\n",
    "    )\n",
    "    COMMENT '{comment}'\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dade522c-0dc0-48b5-a5bc-e5074effe202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"dates.json\", \"r\") as file:\n",
    "    all_holidays = json.load(file)\n",
    "\n",
    "# Insert holidays into the table\n",
    "values = \",\\n        \".join(\n",
    "    [\n",
    "        f\"('{holiday['date']}', '{holiday['type']}', '{holiday['name']}')\"\n",
    "        for holiday in all_holidays\n",
    "    ]\n",
    ")\n",
    "\n",
    "sql = f\"\"\"\n",
    "    INSERT INTO {days_off_table} (Date, Type, Comment)\n",
    "    VALUES \n",
    "        {values}\n",
    "\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0efc9d-65bf-48dc-acd5-17141177afea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fa36394-4364-49bc-987f-1397cf240fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(days_off_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f56f3a-3b8f-4688-bb4d-9e571dd0a7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"SELECT * FROM {days_off_table}\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9d11b8-33ad-424b-ab56-dcecb8e046e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holiday_df = spark.sql(\n",
    "    f\"SELECT Date FROM {days_off_table} WHERE Type != 'Kompensation'\"\n",
    ")\n",
    "holiday_dates = [row.Date for row in holiday_df.collect()]\n",
    "display(holiday_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571625da-6b0a-47ec-98a7-bdbefb692dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "update_query = f\"\"\"\n",
    "    UPDATE {dim_date_table}\n",
    "    SET IsHoliday = True\n",
    "    WHERE Date IN ({','.join([f\"'{date}'\" for date in holiday_dates])})\n",
    "\"\"\"\n",
    "spark.sql(update_query).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d775c0-e1c7-4648-9918-a4d50584f458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "compensation_df = spark.sql(\n",
    "    f\"SELECT Date FROM {days_off_table} WHERE Type = 'Kompensation'\"\n",
    ")\n",
    "compensation_dates = [row.Date for row in compensation_df.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a079d23e-38b2-41c1-b7a2-635d6082d325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# update_query = f\"\"\"\n",
    "#     UPDATE {dim_date_table}\n",
    "#     SET IsCompensation = True\n",
    "#     WHERE Date IN ({','.join([f\"'{date}'\" for date in compensation_dates])})\n",
    "# \"\"\"\n",
    "# spark.sql(update_query).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "148334da-fa94-4f09-ba41-8c2d4720867c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reorder_query = f\"\"\"\n",
    "    SELECT * FROM {dim_date_table}\n",
    "    ORDER BY Date\n",
    "\"\"\"\n",
    "reordered_df = spark.sql(reorder_query)\n",
    "reordered_df.write.mode(\"overwrite\").saveAsTable(dim_date_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a50d5db-c372-482c-8b20-4d862204ce7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a99757c-1e03-493c-bedd-7cc512e20e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"SELECT * FROM {dim_date_table}\"\"\"\n",
    "spark.sql(sql).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eefa586-12cf-4abb-98ce-78367825ddfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_dim_date_time",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
