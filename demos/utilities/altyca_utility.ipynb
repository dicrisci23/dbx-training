{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f2fb66-1ece-4563-986e-2db8ece5ab68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Altyca - Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a5122e3-0470-4112-b445-f6ccb5950ed9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "libraries"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f9ce7a3-1e49-44ea-a486-958bed7fa40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class InMemoryLogHandler(logging.Handler):\n",
    "    \"\"\"\n",
    "    A logging handler that collects logs in memory for later bulk processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name):\n",
    "        super().__init__()\n",
    "        self.logs = []\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def emit(self, record):\n",
    "        log_entry = {\n",
    "            \"timestamp\": self.formatTime(record),\n",
    "            \"level\": record.levelname,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"task\": self.task_name\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "    def formatTime(self, record):\n",
    "        return self.formatter.formatTime(record, \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6628a39d-7105-43ef-89b6-75bb1098e256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    ENVIRONMENTS = {\n",
    "        'dev':  'adb-503590620570631.11.azuredatabricks.net',\n",
    "        'sbx':  'adb-69788370609352.12.azuredatabricks.net',\n",
    "        'uat':  'adb-2874730485941614.14.azuredatabricks.net',\n",
    "        'prd':  'adb-392658947501145.5.azuredatabricks.net',\n",
    "        'tf':   'adb-3544198369222802.2.azuredatabricks.net',\n",
    "        'uc':   'adb-1870340185830510.10.azuredatabricks.net',\n",
    "    }\n",
    "\n",
    "\n",
    "    # Entra groups\n",
    "    ADMINS =        'DTCH-A-APP-U-BI-Admin'\n",
    "    ANALYSTS =      'DTCH-A-APP-U-BI-Analyst'\n",
    "    DEVELOPERS =    'DTCH-A-APP-U-BI-Developer'\n",
    "\n",
    "    # Asset Bundles Service Principals \n",
    "    DAB_SRV_DEV = 'DTCH - BI - DEV - DAB-Srv-DevOp'\n",
    "    DAB_SRV_UAT = 'DTCH - BI - UAT - DAB-Srv-DevOp'\n",
    "    DAB_SRV_PRD = 'DTCH - BI - PRD - DAB-Srv-DevOp'\n",
    "    \n",
    "\n",
    "    # ID' for Asset Bundles Service Principals \n",
    "    DAB_SRV_IDS = {\n",
    "        'dev': '2fedf703-49dc-42e7-95f3-23ab3bd14b4a',\n",
    "        'uat': '352a4a40-ef97-43bc-8c03-e81302510fca',\n",
    "        'prd': '801bcf2a-4c2b-486a-b26d-cb7c150cb733'\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_workspace_id():\n",
    "        \"\"\"\n",
    "        Retrieves the Databricks workspace ID.\n",
    "\n",
    "        Returns:\n",
    "            str: The workspace ID.\n",
    "        \"\"\"\n",
    "        workspace_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().workspaceId().get()\n",
    "        return workspace_id\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current_user():\n",
    "        \"\"\"\n",
    "        Retrieves the current user.\n",
    "\n",
    "        Returns:\n",
    "            str: The current user.\n",
    "        \"\"\"\n",
    "        current_user = spark.sql('SELECT current_user() as user').collect()[0]['user']\n",
    "        return current_user\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_environment():\n",
    "        \"\"\"\n",
    "        Retrieves the environment name based on the current Databricks workspace URL.\n",
    "\n",
    "        Returns:\n",
    "            str: The environment name if the URL matches one in the ENVIRONMENTS dictionary, otherwise, None.\n",
    "        \"\"\"\n",
    "        url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "        for env, env_url in Utility.ENVIRONMENTS.items():\n",
    "            if url == env_url:\n",
    "                return env\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dab_srv_id(environment):\n",
    "        \"\"\"\n",
    "        Retrieves the Service Principal ID for DAB Deployment based on the specified environment.\n",
    "\n",
    "        Args:\n",
    "            environment (str): The environment for which to retrieve the Service Principal ID.\n",
    "\n",
    "        Returns:\n",
    "            str: The Service Principal ID for the specified environment, or None if not found.\n",
    "        \"\"\"\n",
    "       \n",
    "        for env, id in Utility.DAB_SRV_IDS.items():\n",
    "            if env == environment:\n",
    "                return id\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_logger(name=__name__, level=logging.INFO, task_name=None):\n",
    "        \"\"\"\n",
    "        Creates a logger that logs to both the notebook stdout and in-memory (for Delta write).\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(name)\n",
    "\n",
    "        if task_name is None:\n",
    "            task_name = name\n",
    "\n",
    "        # Clear previous handlers\n",
    "        if logger.hasHandlers():\n",
    "            logger.handlers.clear()\n",
    "\n",
    "        logger.setLevel(level)\n",
    "        logger.propagate = False\n",
    "\n",
    "        # Stream Handler (stdout)\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "        # In-memory Handler\n",
    "        memory_handler = InMemoryLogHandler(task_name=task_name)\n",
    "        memory_handler.setFormatter(formatter)\n",
    "        logger.addHandler(memory_handler)\n",
    "\n",
    "        # Attach to logger for access later\n",
    "        logger.memory_handler = memory_handler\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def flush_logs_to_delta(logger, pipeline, task, job_run_id, table_name):\n",
    "        \"\"\"\n",
    "        Writes collected in-memory logs to the given Delta Table.\n",
    "        \"\"\"\n",
    "        logs = getattr(logger, \"memory_handler\", None)\n",
    "        if logs is None or not logs.logs:\n",
    "            return\n",
    "\n",
    "        rows = []\n",
    "        for log in logs.logs:\n",
    "            rows.append((\n",
    "                datetime.strptime(log[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\"),  # assuming timestamp format is correct\n",
    "                log[\"level\"],\n",
    "                log[\"message\"],\n",
    "                pipeline, \n",
    "                task,\n",
    "                job_run_id\n",
    "            ))\n",
    "\n",
    "        # Updated schema to match Delta Table schema\n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"level\", StringType(), True),\n",
    "            StructField(\"message\", StringType(), True),\n",
    "            StructField(\"pipeline\", StringType(), True),  # \"logger\" field added\n",
    "            StructField(\"task\", StringType(), True),  # \"task_name\" instead of \"task\"\n",
    "            StructField(\"job_run_id\", StringType(), True) \n",
    "        ])\n",
    "\n",
    "        # Create DataFrame with the correct schema\n",
    "        df = spark.createDataFrame(rows, schema=schema)\n",
    "        \n",
    "        # Write to Delta Table\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "    @staticmethod    \n",
    "    def close_logger(logger):\n",
    "        \"\"\"\n",
    "        Close all handlers attached to the logger.\n",
    "        \"\"\"\n",
    "        for handler in logger.handlers:\n",
    "            handler.close()\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "    @staticmethod\n",
    "    def cast_all_ntz_timestamps_to_classic(df):\n",
    "        \"\"\"\n",
    "        Casts all timestamp_ntz columns to classic Spark 'timestamp' type\n",
    "        (UTC-based with time zone support) to avoid Delta feature errors.\n",
    "        \"\"\"\n",
    "        for field in df.schema.fields:\n",
    "            # Use simpleString() to catch timestamp_ntz\n",
    "            if field.dataType.simpleString() == \"timestamp_ntz\":\n",
    "                df = df.withColumn(field.name, col(field.name).cast(\"timestamp\"))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "648cbf74-72fd-4ce8-bcce-665c7d558dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('class Utility sucessfully loaded')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "altyca_utility",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
